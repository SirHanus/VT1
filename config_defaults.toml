# Default configuration for VT1 (edit config_local.toml to override)
models_dir = "models"                           # Root folder where model files live; other model paths are resolved relative to this
pose_model = "models/yolo11x-pose.pt"            # Default YOLO pose model used in the pipeline for keypoints; change to switch pose backbone
yolo_model = "models/yolo11n.pt"                 # Default YOLO detection model (used in fallback detection and eval); change to alter detector size/accuracy
# Trained team clustering models (umap.pkl, kmeans.pkl) default to outputs
team_models_dir = "models/team_clustering"       # Folder where UMAP/KMeans .pkl are saved/loaded; clustering saves here, eval/pipeline read from here
# Root-level outputs folder
pipeline_output_dir = "outputs"                  # Top-level folder for pipeline run outputs (videos, JSON, images)
# Separate output subtree for team clustering utilities (build/eval/audit)
team_output_dir = "outputs/team_clustering"      # Working folder for team clustering tools (training set, clustering artifacts, eval results)
log_level = "INFO"                                # Global log level (DEBUG, INFO, WARNING, ERROR); higher verbosity helps debugging

# --- Shared defaults used by CLI and GUI ---
# Detection / inference
yolo_conf = 0.30                                  # YOLO confidence threshold; higher = fewer, more confident detections; lower = more, noisier detections
yolo_imgsz = 640                                   # YOLO inference size; larger = slower but potentially more accurate detections
central_ratio_default = 0.6                        # Fraction of the bbox kept around center for crops; lower zooms in on jerseys, higher keeps more context
siglip_model = "google/siglip-base-patch16-224"  # SigLIP vision model id (HF); changing affects embedding dimensionality/quality and VRAM needs
person_class_name = "person"                      # MMDetection class name to treat as players; change if your detector uses a different label

# Build training set sampling
training_videos_dir = "videos_all/CAR_vs_NYR"     # Default folder scanned for videos when building the training set
videos_glob = "*.mp4"                             # Glob pattern used under training_videos_dir; narrow or widen to select files
build_fps = 1.0                                    # Frame sampling rate in FPS; higher collects more crops and increases processing time
build_min_crop_size = 32                           # Minimum crop size in pixels; smaller allows tiny detections, larger filters out small players
build_batch_size = 64                              # Batch size for SigLIP embedding; higher is faster but uses more VRAM/RAM
det_score_thr_default = 0.30                       # Detector score threshold during build; higher yields fewer crops, lower yields more (with noise)

# Clustering defaults
cluster_k = 2                                      # Number of KMeans clusters (teams); 2 for home/away; change if your dataset needs more groups
umap_dim = 16                                      # Target dimensionality after UMAP; 2 for plotting, 16+ for training KMeans with more structure
umap_neighbors = 15                                # UMAP n_neighbors; lower = more local detail, higher = more global structure
umap_metric = "cosine"                            # UMAP distance metric; cosine works well for normalized embeddings like SigLIP
umap_min_dist = 0.1                                # UMAP min_dist; lower packs clusters tighter (can improve separation but may overfit)
random_seed = 0                                    # Global seed for reproducible sampling and clustering; change to get different cluster initializations

# Evaluation defaults
eval_frame_step = 30                               # Sample one frame every N frames when evaluating on videos; lower = more frames, slower
eval_limit_images = 50                             # Max number of annotated images to save per eval run; prevents dumping too many files
yolo_max_boxes = 8                                 # Max boxes to annotate per image/frame in eval; limits clutter and speeds up drawing
